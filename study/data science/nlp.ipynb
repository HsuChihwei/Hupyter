{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jaccard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "# '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "def replace_punctuation(item):\n",
    "    for i in string.punctuation.replace(\"''\", ''):\n",
    "        if i in item:\n",
    "            item = item.replace(i, '')\n",
    "    return item\n",
    "\n",
    "tokenize = lambda doc: replace_punctuation(doc.lower()).split(\" \")\n",
    "# tokenize = lambda doc: doc.lower().split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_0 = \"China has a strong economy that is growing at a rapid pace. However politically it differs greatly from the US Economy.\"\n",
    "document_1 = \"At last, China seems serious about confronting an endemic problem: domestic violence and corruption.\"\n",
    "document_2 = \"Japan's prime minister, Shinzo Abe, is working towards healing the economic turmoil in his own country for his view on the future of his people.\"\n",
    "document_3 = \"Vladimir Putin is working hard to fix the economy in Russia as the Ruble has tumbled.\"\n",
    "document_4 = \"What's the future of Abenomics? We asked Shinzo Abe for his views\"\n",
    "document_5 = \"Obama has eased sanctions on Cuba while accelerating those against the Russian Economy, even as the Ruble's value falls almost daily.\"\n",
    "document_6 = \"Vladimir Putin is riding a horse while hunting deer. Vladimir Putin always seems so serious about things - even riding horses. Is he crazy?\"\n",
    "\n",
    "all_documents = [document_0, document_1, document_2, document_3, document_4, document_5, document_6]\n",
    "\n",
    "tokenized_documents = [tokenize(d) for d in all_documents] # tokenized docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'at', 'endemic', 'confronting', 'china', 'about', 'violence', 'an', 'corruption', 'domestic', 'seems', 'serious', 'problem', 'last', 'and'} {'', 'is', 'hunting', 'a', 'while', 'he', 'always', 'horse', 'even', 'about', 'seems', 'serious', 'horses', 'things', 'deer', 'riding', 'so', 'crazy', 'putin', 'vladimir'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0967741935483871"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def jaccard_similarity(query, document):\n",
    "    print(set(query), set(document))\n",
    "    intersection = set(query).intersection(set(document))\n",
    "#     print(intersection)\n",
    "    union = set(query).union(set(document))\n",
    "    return len(intersection)/len(union)\n",
    "\n",
    "jaccard_similarity(tokenized_documents[1],tokenized_documents[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "- Term Frequency \n",
    "- Inverse Document Frequency\n",
    "- tf-idf = tf*idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.9459101490553135, 0.0, 2.9459101490553135, 0.0, 0.0, 0.0, 0.0, 0.0, 2.252762968495368, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.252762968495368, 0.0, 2.9459101490553135, 0.0, 2.9459101490553135, 0.0, 0.0, 0.0, 2.9459101490553135, 2.9459101490553135, 0.0, 0.0, 0.0, 0.0, 0.0, 2.9459101490553135, 0.0, 0.0, 2.9459101490553135, 0.0, 3.8142592685777856, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.1277471639690138, 2.9459101490553135, 0.0, 0.0, 1.5596157879354227, 2.9459101490553135, 0.0, 2.9459101490553135, 0.0, 0.0, 0.0, 0.0, 1.336472236621213, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.9459101490553135, 0.0, 0.0, 0.0, 1.8472978603872037]\n",
      "China has a strong economy that is growing at a rapid pace. However politically it differs greatly from the US Economy.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def term_frequency(term, tokenized_document):\n",
    "    return tokenized_document.count(term)\n",
    "\n",
    "def sublinear_term_frequency(term, tokenized_document):\n",
    "#     return math.log(1+(tokenized_document.count(term)/len(tokenized_document)))\n",
    "    return (1 + math.log(tokenized_document.count(term))) if tokenized_document.count(term) else 0\n",
    "\n",
    "def augmented_term_frequency(term, tokenized_document):\n",
    "    max_count = max([term_frequency(t, tokenized_document) for t in tokenized_document])\n",
    "    return (0.5 + ((0.5 * term_frequency(term, tokenized_document))/max_count))\n",
    "\n",
    "def inverse_document_frequencies(tokenized_documents):\n",
    "    idf_values = {}\n",
    "    all_tokens_set = set([item for sublist in tokenized_documents for item in sublist])\n",
    "    for tkn in all_tokens_set:\n",
    "        contains_token = map(lambda doc: tkn in doc, tokenized_documents)\n",
    "        idf_values[tkn] = 1 + math.log(len(tokenized_documents)/(sum(contains_token)))\n",
    "    return idf_values\n",
    "\n",
    "def tfidf(documents):\n",
    "    tokenized_documents = [tokenize(d) for d in documents]\n",
    "    idf = inverse_document_frequencies(tokenized_documents)\n",
    "    tfidf_documents = []\n",
    "    for document in tokenized_documents:\n",
    "        doc_tfidf = []\n",
    "        for term in idf.keys():\n",
    "            tf = sublinear_term_frequency(term, document)\n",
    "            doc_tfidf.append(tf * idf[term])\n",
    "        tfidf_documents.append(doc_tfidf)\n",
    "    return tfidf_documents\n",
    "\n",
    "tfidf_representation = tfidf(all_documents)\n",
    "print(tfidf_representation[0])\n",
    "print(document_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.9459101490553135, 0.0, 2.9459101490553135, 0.0, 0.0, 0.0, 0.0, 0.0, 2.252762968495368, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.252762968495368, 0.0, 2.9459101490553135, 0.0, 2.9459101490553135, 0.0, 0.0, 0.0, 2.9459101490553135, 2.9459101490553135, 0.0, 0.0, 0.0, 0.0, 0.0, 2.9459101490553135, 0.0, 0.0, 2.9459101490553135, 0.0, 3.8142592685777856, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.1277471639690138, 2.9459101490553135, 0.0, 0.0, 1.5596157879354227, 2.9459101490553135, 0.0, 2.9459101490553135, 0.0, 0.0, 0.0, 0.0, 1.336472236621213, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.9459101490553135, 0.0, 0.0, 0.0, 1.8472978603872037]\n",
      "[0.0, 0.3153972256619837, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18627868225707217, 0.18627868225707217, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2435943187490687, 0.0, 0.0, 0.0, 0.25863023686268505, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2435943187490687, 0.0, 0.2435943187490687, 0.2435943187490687, 0.0, 0.15275118420428915, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2435943187490687, 0.0, 0.0, 0.12896304576507567, 0.2435943187490687, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2435943187490687, 0.0, 0.2435943187490687, 0.0, 0.0, 0.0, 0.2435943187490687, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2435943187490687, 0.2435943187490687, 0.11051153210195065, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2435943187490687, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "China has a strong economy that is growing at a rapid pace. However politically it differs greatly from the US Economy.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "sklearn_tfidf = TfidfVectorizer(norm='l2',min_df=0, use_idf=True, smooth_idf=False, sublinear_tf=True, tokenizer=tokenize)\n",
    "\n",
    "sklearn_representation = sklearn_tfidf.fit_transform(all_documents)\n",
    "print(tfidf_representation[0])\n",
    "print(sklearn_representation.toarray()[0].tolist())\n",
    "print(document_0)\n",
    "# print(sklearn_tfidf.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vector1, vector2):\n",
    "    dot_product = sum(p*q for p,q in zip(vector1, vector2))\n",
    "    magnitude = math.sqrt(sum([val**2 for val in vector1])) * math.sqrt(sum([val**2 for val in vector2]))\n",
    "    if not magnitude:\n",
    "        return 0\n",
    "    return dot_product/magnitude\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1.0000000000000002, 4, 4), (1.0000000000000002, 6, 6))\n",
      "((1.0000000000000002, 2, 2), (1.0000000000000002, 5, 5))\n",
      "((1.0, 5, 5), (1.0000000000000002, 4, 4))\n",
      "((1.0, 3, 3), (1.0000000000000002, 0, 0))\n",
      "((1.0, 0, 0), (1.0, 3, 3))\n",
      "((0.9999999999999999, 6, 6), (1.0, 2, 2))\n",
      "((0.9999999999999999, 1, 1), (1.0, 1, 1))\n",
      "((0.3485485947359029, 4, 2), (0.34854859473590283, 4, 2))\n",
      "((0.3485485947359029, 2, 4), (0.34854859473590283, 2, 4))\n",
      "((0.1665505800294338, 6, 3), (0.16655058002943382, 6, 3))\n",
      "((0.1665505800294338, 3, 6), (0.16655058002943382, 3, 6))\n",
      "((0.14747371149061467, 5, 3), (0.14747371149061467, 5, 3))\n",
      "((0.14747371149061467, 3, 5), (0.14747371149061467, 3, 5))\n",
      "((0.14340231078604393, 3, 2), (0.1434023107860439, 3, 2))\n",
      "((0.14340231078604393, 2, 3), (0.1434023107860439, 2, 3))\n",
      "((0.12639252989744235, 3, 0), (0.12639252989744235, 3, 0))\n",
      "((0.12639252989744235, 0, 3), (0.12639252989744235, 0, 3))\n",
      "((0.1121220817608579, 6, 1), (0.11212208176085793, 6, 1))\n",
      "((0.1121220817608579, 1, 6), (0.11212208176085793, 1, 6))\n",
      "((0.08385854691703915, 5, 0), (0.08385854691703916, 5, 0))\n",
      "((0.08385854691703915, 0, 5), (0.08385854691703916, 0, 5))\n",
      "((0.08250495926004221, 1, 0), (0.08250495926004221, 1, 0))\n",
      "((0.08250495926004221, 0, 1), (0.08250495926004221, 0, 1))\n",
      "((0.07874012359805513, 6, 0), (0.07874012359805514, 6, 0))\n",
      "((0.07874012359805513, 0, 6), (0.07874012359805514, 0, 6))\n",
      "((0.06570754598243626, 5, 2), (0.06570754598243628, 5, 2))\n",
      "((0.06570754598243626, 2, 5), (0.06570754598243628, 2, 5))\n",
      "((0.06313181256696991, 6, 5), (0.06313181256696994, 6, 5))\n",
      "((0.06313181256696991, 5, 6), (0.06313181256696994, 5, 6))\n",
      "((0.036290965195590316, 4, 3), (0.03629096519559032, 4, 3))\n",
      "((0.036290965195590316, 3, 4), (0.03629096519559032, 3, 4))\n",
      "((0.03502379133231999, 2, 0), (0.03502379133231999, 2, 0))\n",
      "((0.03502379133231999, 0, 2), (0.03502379133231999, 0, 2))\n",
      "((0.028873126228785236, 5, 4), (0.02887312622878524, 5, 4))\n",
      "((0.028873126228785236, 4, 5), (0.02887312622878524, 4, 5))\n",
      "((0.023949059277239986, 6, 2), (0.023949059277239993, 6, 2))\n",
      "((0.023949059277239986, 2, 6), (0.023949059277239993, 2, 6))\n",
      "((0.016983491902926594, 4, 0), (0.01698349190292659, 4, 0))\n",
      "((0.016983491902926594, 0, 4), (0.01698349190292659, 0, 4))\n",
      "((0.0, 6, 4), (0.0, 6, 4))\n",
      "((0.0, 5, 1), (0.0, 5, 1))\n",
      "((0.0, 4, 6), (0.0, 4, 6))\n",
      "((0.0, 4, 1), (0.0, 4, 1))\n",
      "((0.0, 3, 1), (0.0, 3, 1))\n",
      "((0.0, 2, 1), (0.0, 2, 1))\n",
      "((0.0, 1, 5), (0.0, 1, 5))\n",
      "((0.0, 1, 4), (0.0, 1, 4))\n",
      "((0.0, 1, 3), (0.0, 1, 3))\n",
      "((0.0, 1, 2), (0.0, 1, 2))\n"
     ]
    }
   ],
   "source": [
    "tfidf_representation = tfidf(all_documents)\n",
    "our_tfidf_comparisons = []\n",
    "for count_0, doc_0 in enumerate(tfidf_representation):\n",
    "    for count_1, doc_1 in enumerate(tfidf_representation):\n",
    "        our_tfidf_comparisons.append((cosine_similarity(doc_0, doc_1), count_0, count_1))\n",
    "\n",
    "skl_tfidf_comparisons = []\n",
    "for count_0, doc_0 in enumerate(sklearn_representation.toarray()):\n",
    "    for count_1, doc_1 in enumerate(sklearn_representation.toarray()):\n",
    "        skl_tfidf_comparisons.append((cosine_similarity(doc_0, doc_1), count_0, count_1))\n",
    "\n",
    "for x in zip(sorted(our_tfidf_comparisons, reverse = True), sorted(skl_tfidf_comparisons, reverse = True)):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity_bqc(str1, str2):\n",
    "    set_1 = set(str1)\n",
    "    set_2 = set(str2)\n",
    "    set_3 = set_1.intersection(set_2)\n",
    "    n = len(set_3)\n",
    "    if len(set_1) == 0 or len(set_2) == 0:\n",
    "        return 0\n",
    "    if str1 == str2:\n",
    "        return 1\n",
    "    if (str1 in str2 and len(str1) >= 4)or (str2 in str1 and len(str2) >= 4):\n",
    "        return 0.8\n",
    "    sm_pr = n / float(len(set_1) + len(set_2) - n)\n",
    "    if n == float(len(set_1) + len(set_2) - n):\n",
    "        return 1\n",
    "    else:\n",
    "        return sm_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['秒 贷 金融', '秒 贷网']\n",
      "True\n",
      "0.0\n",
      "{'贷', '融', '秒', '金'} {'贷', '秒', '网'}\n",
      "0.4\n",
      "0.4\n"
     ]
    }
   ],
   "source": [
    "import jieba,math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    dot_product = sum(p*q for p,q in zip(vector1, vector2))\n",
    "    magnitude = math.sqrt(sum([val**2 for val in vector1])) * math.sqrt(sum([val**2 for val in vector2]))\n",
    "    if not magnitude:\n",
    "        return 0\n",
    "    return dot_product/magnitude\n",
    "\n",
    "def cosine_similarity_bqc(str1, str2):\n",
    "    if len(str1) == 0 or len(str2) == 0:\n",
    "        return 0.0\n",
    "    if str1 == str2:\n",
    "        return 1.0\n",
    "    if (str1 in str2 and len(str1) >= 4)or (str2 in str1 and len(str2) >= 4):\n",
    "        return 0.8\n",
    "    tmp = [\" \".join(jieba.cut(i)) for i in [str1, str2]]\n",
    "    print(tmp)\n",
    "    print(tmp[0]<tmp[1])\n",
    "    sklearn_tfidf = TfidfVectorizer(norm='l2',min_df=0, use_idf=True, smooth_idf=False, sublinear_tf=True)\n",
    "    sklearn_representation = sklearn_tfidf.fit_transform(tmp)\n",
    "    sm = cosine_similarity(sklearn_representation.toarray()[0].tolist(),sklearn_representation.toarray()[1].tolist())\n",
    "    return sm\n",
    "\n",
    "a = '秒贷金融'\n",
    "b = '秒贷网'\n",
    "print(cosine_similarity_bqc(a, b))\n",
    "print(jaccard_similarity(a, b))\n",
    "print(jaccard_similarity_bqc(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'福', '东', '北', '子', '饺'} {'福', '多', '东', '馆', '北', '味', '子', '饺'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = set('东北福饺子')\n",
    "b = set('东北福多味饺子馆')\n",
    "print(a,b)\n",
    "b>a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29606"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3773 + 3855 + 2603 + 3755 + 3344 + 3568 + 2724 + 3196 + 2788"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
